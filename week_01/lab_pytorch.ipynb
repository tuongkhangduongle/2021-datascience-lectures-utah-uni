{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "efJ4mmf7ppmi",
        "egjOffpApFQ_",
        "Y6xCwmF3pLkK",
        "NCJlcmX1piAG",
        "YkugLAv7Dd-B",
        "31XDyaJsq1Ok",
        "FHdvSlu4rP_b",
        "i6P5kXvHri4P",
        "DB2HCOUNtCKl",
        "A3IUafHbtMNg",
        "Mt1Ek4XlDhUk",
        "i_5lRfZRDky2",
        "z9OmRODdDnc9",
        "WTACVxjSDqme",
        "Ch_Bk6VoB6g_",
        "hA2v43rsGVSy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuongkhangduongle/2021-datascience-lectures-utah-uni/blob/master/week_01/lab_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PYTORCH TUTORIAL"
      ],
      "metadata": {
        "id": "LeWkHeQCIXD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Demo dataset - MNIST"
      ],
      "metadata": {
        "id": "efJ4mmf7ppmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Download data"
      ],
      "metadata": {
        "id": "egjOffpApFQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the classic MNIST dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
        "\n",
        "We will use `pathlib` for dealing with paths (part of the Python 3 standard library), and will download the dataset using requests. We will only import modules when we use them, so you can see exactly what’s being used at each point."
      ],
      "metadata": {
        "id": "hXyY1aoEptOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "  ##################\n",
        "  # Download dataset\n",
        "  # CODE HERE\n",
        "  content = requests.get(URL+FILENAME).content\n",
        "  (PATH / FILENAME).open('wb').write(content)\n",
        "  #\n",
        "  ##################"
      ],
      "metadata": {
        "id": "fcg1eDnKp4am"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data."
      ],
      "metadata": {
        "id": "_F3YI788p742"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "  ##################\n",
        "  # Load dataset\n",
        "  # CODE HERE\n",
        "  ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding = 'latin-1')\n",
        "  \n",
        "  #\n",
        "  ##################"
      ],
      "metadata": {
        "id": "Z1CrgPKnp9yW"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualize data"
      ],
      "metadata": {
        "id": "Y6xCwmF3pLkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28). Let’s take a look at one; we need to reshape it to 2d first."
      ],
      "metadata": {
        "id": "-AR2POOxqE1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "##################\n",
        "# Use matplotlib to visualize sample image\n",
        "# CODE HERE\n",
        "pyplot.imshow(x_train[0].reshape((28,28)), cmap = 'gray')\n",
        "print('label: ', y_train[0])\n",
        "\n",
        "#\n",
        "##################"
      ],
      "metadata": {
        "id": "_twvNpleqEH2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "fb0ea219-e0f2-4fee-f1af-52879f4db4b1"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "# Show train shape\n",
        "# CODE HERE\n",
        "print(x_train.shape)\n",
        "\n",
        "#\n",
        "##################"
      ],
      "metadata": {
        "id": "wOw0f6bRpX4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3906640d-5275-443a-f39a-cd33112294b5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 PyTorch Tensor"
      ],
      "metadata": {
        "id": "NCJlcmX1piAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch uses `torch.tensor`, rather than numpy arrays, so we need to convert our data."
      ],
      "metadata": {
        "id": "35wwkKfPqKUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "##################\n",
        "# Convert numpy array to torch.tensor\n",
        "# CODE \n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")\n",
        "n,c = x_train.shape\n",
        "\n",
        "#\n",
        "##################"
      ],
      "metadata": {
        "id": "qHwvLgfcqMN9"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "# Show torch.tensor properties\n",
        "# CODE HERE\n",
        "print(\"xtrain: \", x_train)\n",
        "print('ytrain', y_train)\n",
        "print('shape of x_train:', x_train.shape)\n",
        "print()\n",
        "print(y_train.min(), y_train.max())\n",
        "\n",
        "#\n",
        "##################"
      ],
      "metadata": {
        "id": "v0Bp8WnTDs9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c7b591-bf8d-463b-be1b-5cf3b05f5f14"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xtrain:  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "ytrain tensor([5, 0, 4,  ..., 8, 4, 8])\n",
            "shape of x_train: torch.Size([50000, 784])\n",
            "\n",
            "tensor(0) tensor(9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural net from scratch (no torch.nn)"
      ],
      "metadata": {
        "id": "YkugLAv7Dd-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create a simple linear model"
      ],
      "metadata": {
        "id": "31XDyaJsq1Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n",
        "\n",
        "For the weights, we set `requires_grad` **after** the initialization, since we don’t want that step included in the gradient. (Note that a trailing `_` in PyTorch signifies that the operation is performed in-place.)"
      ],
      "metadata": {
        "id": "0QRjXPF8qhed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "KRnpFROywz5B"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to PyTorch’s ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model! So let’s just write a plain matrix multiplication and broadcasted addition to create a simple linear model. We also need an activation function, so we’ll write `log_softmax` and use it.\n",
        "\n",
        "**Remember:** although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically."
      ],
      "metadata": {
        "id": "E119ZRQqqw7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_softmax(x):\n",
        "  ##################\n",
        "  # Use pytorch function to create softmax function\n",
        "  # CODE HERE\n",
        "  return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "  #\n",
        "  ##################\n",
        "\n",
        "def model(xb):\n",
        "  return log_softmax(xb @ weights + bias)\n",
        "# @ operation for produce of 2 matrixes"
      ],
      "metadata": {
        "id": "GypZttILqxbE"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above, the `@` stands for the matrix multiplication operation."
      ],
      "metadata": {
        "id": "yFzAtFJFq0mT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Predict"
      ],
      "metadata": {
        "id": "FHdvSlu4rP_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will call our function on one batch of data (in this case, 64 images). This is one forward pass. Note that our predictions won’t be any better than random at this stage, since we start with random weights."
      ],
      "metadata": {
        "id": "Z5NIVB2_DP2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64  # batch size\n",
        "\n",
        "xb = x_train[0:bs]  # a mini-batch from x\n",
        "preds = model(xb)  # predictions\n",
        "print(preds[0], preds.shape)"
      ],
      "metadata": {
        "id": "AN0YzZPzq1Jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45534a82-dba0-41a3-c529-066b2e64ce2e"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.2710, -1.6262, -2.3080, -2.3935, -2.5060, -2.3691, -2.3058, -2.8403,\n",
            "        -2.4560, -2.4043], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the `preds` tensor contains not only the tensor values, but also a gradient function. We’ll use this later to do backprop."
      ],
      "metadata": {
        "id": "05AdLdpOq4Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Loss"
      ],
      "metadata": {
        "id": "i6P5kXvHri4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s implement negative log-likelihood to use as the loss function (again, we can just use standard Python):"
      ],
      "metadata": {
        "id": "a_zZZ7N6rnF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nll(input, target):\n",
        "  return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = nll"
      ],
      "metadata": {
        "id": "8GkCB69Uq8JU"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check our loss with our random model, so we can see if we improve after a backprop pass later."
      ],
      "metadata": {
        "id": "oRfwiENEq8qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yb = y_train[0:bs]\n",
        "print(loss_func(preds, yb))"
      ],
      "metadata": {
        "id": "UFr-VAPAq-FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62b839a-da64-4bb1-ff89-50d6b3e7c81d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2465, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Evaluate"
      ],
      "metadata": {
        "id": "DB2HCOUNtCKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s also implement a function to calculate the accuracy of our model. For each prediction, if the index with the largest value matches the target value, then the prediction was correct."
      ],
      "metadata": {
        "id": "c-A6IEylrCPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(out, yb):\n",
        "  ##################\n",
        "  # Use pytorch function to create accuracy function\n",
        "  # CODE HERE\n",
        "  cnt = 0\n",
        "  for i in range(len(yb)):\n",
        "      if torch.argmax(out[i]).item() == yb[i].item():\n",
        "          cnt+=1\n",
        "  return cnt/len(yb)\n",
        "  #\n",
        "  ##################"
      ],
      "metadata": {
        "id": "WtJ4kUr5rDrS"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s check the accuracy of our random model, so we can see if our accuracy improves as our loss improves."
      ],
      "metadata": {
        "id": "XsbXX-hsrFwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(preds, yb))"
      ],
      "metadata": {
        "id": "8qE1hoOOrGPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d06c55-430d-4474-a1c8-316f40ab55fa"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Training loop"
      ],
      "metadata": {
        "id": "A3IUafHbtMNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run a training loop. For each iteration, we will:\n",
        "\n",
        "- select a mini-batch of data (of size bs)\n",
        "- use the model to make predictions\n",
        "- calculate the loss\n",
        "- `loss.backward()` updates the gradients of the model, in this case, weights and bias.\n",
        "\n",
        "We now use these gradients to update the weights and bias. We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch’s Autograd records operations here.\n",
        "\n",
        "We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. `loss.backward()` adds the gradients to whatever is already stored, rather than replacing them)."
      ],
      "metadata": {
        "id": "P6I1FvJFrMiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tip:** You can use the standard python debugger to step through PyTorch code, allowing you to check the various variable values at each step. Uncomment set_trace() below to try it out."
      ],
      "metadata": {
        "id": "-Fh9AoKo-Y7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "lr = 0.5  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    # set_trace()\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "  \n",
        "    pred = model(xb)\n",
        "  \n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "  \n",
        "    with torch.no_grad():\n",
        "      ##################\n",
        "      # Update weights and bias by gradient descent algorithm\n",
        "      # CODE HERE\n",
        "      \n",
        "      weights -= weights.grad * lr\n",
        "      bias -= bias.grad *lr\n",
        "      weights.grad.zero_() \n",
        "      bias.grad.zero_()\n",
        "      #\n",
        "      ##################"
      ],
      "metadata": {
        "id": "1EXayB_OraMY"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s it: we’ve created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch!\n",
        "\n",
        "Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."
      ],
      "metadata": {
        "id": "ZCS-98k_regg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "metadata": {
        "id": "zOIDYQL7rfqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f3f17f-bda0-4e58-8d27-430f324b4e22"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0661, grad_fn=<NllLossBackward0>) 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Using torch.nn.functional"
      ],
      "metadata": {
        "id": "Mt1Ek4XlDhUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now refactor our code, so that it does the same thing as before, only we’ll start taking advantage of PyTorch’s `nn` classes to make it more concise and flexible. At each step from here, we should be making our code one or more of: shorter, more understandable, and/or more flexible.\n",
        "\n",
        "The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from `torch.nn.functional` (which is generally imported into the namespace `F` by convention). This module contains all the functions in the `torch.nn` library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you’ll also find here some convenient functions for creating neural nets, such as pooling functions. (There are also functions for doing convolutions, linear layers, etc, but as we’ll see, these are usually better handled using other parts of the library.)\n",
        "\n",
        "If you’re using negative log likelihood loss and log softmax activation, then Pytorch provides a single function `F.cross_entropy` that combines the two. So we can even remove the activation function from our model."
      ],
      "metadata": {
        "id": "aN1FOnjzsDvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "  ##################\n",
        "  # Create simple fully connected neural network\n",
        "  # CODE HERE\n",
        "  return xb@weights + bias\n",
        "  #\n",
        "  ##################"
      ],
      "metadata": {
        "id": "N6oK6qyuDvk9"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we no longer call `log_softmax` in the `model` function. Let’s confirm that our loss and accuracy are the same as before:"
      ],
      "metadata": {
        "id": "UXGeECFysWcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "metadata": {
        "id": "jAm_eP8KsUoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03baa95b-3a37-4ce5-a67d-6456a54c5db5"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0661, grad_fn=<NllLossBackward0>) 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using torch.nn.Module"
      ],
      "metadata": {
        "id": "i_5lRfZRDky2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up, we'll use `nn.Module` and `nn.Parameter`, for a clearer and more\n",
        "concise training loop. We subclass `nn.Module` (which itself is a class and\n",
        "able to keep track of state). In this case, we want to create a class that\n",
        "holds our weights, bias, and method for the forward step. `nn.Module` has a\n",
        "number of attributes and methods (such as `.parameters()` and `.zero_grad()`)\n",
        "which we will be using."
      ],
      "metadata": {
        "id": "usZUNXfN-vH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    ##################\n",
        "    # Create random weights and bias\n",
        "    # CODE HERE\n",
        "    self.weights = nn.Parameter(torch.randn(784,10)/math.sqrt(784))\n",
        "    self.bias = nn.Parameter(torch.zeros(10))\n",
        "    #\n",
        "    ##################\n",
        "\n",
        "  def forward(self, xb):\n",
        "    ##################\n",
        "    # Create forward function\n",
        "    # CODE HERE\n",
        "    return xb@self.weights + self.bias\n",
        "    #\n",
        "    ##################"
      ],
      "metadata": {
        "id": "_GEKBhTsDwOd"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're now using an object instead of just using a function, we\n",
        "first have to instantiate our model:"
      ],
      "metadata": {
        "id": "THdT5-fD_IJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Mnist_Logistic()"
      ],
      "metadata": {
        "id": "rgtmOoEH_Kzb"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can calculate the loss in the same way as before. Note that\n",
        "`nn.Module` objects are used as if they are functions (i.e they are\n",
        "*callable*), but behind the scenes Pytorch will call our `forward`\n",
        "method automatically."
      ],
      "metadata": {
        "id": "za5K5RKl_Mrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "Ar8Pim05_RWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae643f0-89a9-4861-c1bc-255f9b366915"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4809, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously for our training loop we had to update the values for each parameter\n",
        "by name, and manually zero out the grads for each parameter separately, like this:\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "  weights -= weights.grad * lr\n",
        "  bias -= bias.grad * lr\n",
        "  weights.grad.zero_()\n",
        "  bias.grad.zero_()\n",
        "```\n",
        "\n",
        "Now we can take advantage of `model.parameters()` and `model.zero_grad()` (which\n",
        "are both defined by PyTorch for `nn.Module`) to make those steps more concise\n",
        "and less prone to the error of forgetting some of our parameters, particularly\n",
        "if we had a more complicated model:\n",
        "\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "  for p in model.parameters(): p -= p.grad * lr\n",
        "  model.zero_grad()\n",
        "```\n",
        "\n",
        "\n",
        "We'll wrap our little training loop in a `fit` function so we can run it\n",
        "again later.\n",
        "\n"
      ],
      "metadata": {
        "id": "TQqM99ME_WTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit():\n",
        "  for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "      start_i = i * bs\n",
        "      end_i = start_i + bs\n",
        "      xb = x_train[start_i:end_i]\n",
        "      yb = y_train[start_i:end_i]\n",
        "      pred = model(xb)\n",
        "      loss = loss_func(pred, yb)\n",
        "\n",
        "      loss.backward()\n",
        "      with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "          ##################\n",
        "          # Update weights and bias\n",
        "          # CODE HERE\n",
        "          p -= p.grad * lr\n",
        "          pass\n",
        "          #\n",
        "          ##################\n",
        "        model.zero_grad()\n",
        "\n",
        "fit()"
      ],
      "metadata": {
        "id": "8ZmjFMSl_pPf"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "YOHgpc2w_use",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2186d362-6714-4390-ea2d-fc68bcb19e6a"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0647, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We continue to refactor our code.  Instead of manually defining and\n",
        "initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n",
        "self.weights + self.bias``, we will instead use the Pytorch class\n",
        "`nn.Linear <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ for a\n",
        "linear layer, which does all that for us. Pytorch has many types of\n",
        "predefined layers that can greatly simplify our code, and often makes it\n",
        "faster too."
      ],
      "metadata": {
        "id": "5W1iCR_5_3wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_Logistic_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    ##################\n",
        "    # Use torch.nn.Linear instead of weights and bias\n",
        "    # CODE HERE\n",
        "        self.lin == nn.Linear(784, 10)\n",
        "    #\n",
        "    ##################\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb)"
      ],
      "metadata": {
        "id": "0YpJoy0q_5Sy"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate our model and calculate the loss in the same way as before:"
      ],
      "metadata": {
        "id": "-Pa3VKk___Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Mnist_Logistic_v2()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "uOkfHtSmAAtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "b81baa5f-8863-4a2b-b2ec-62ccb5c3a37c"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-195-e174c783b80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMnist_Logistic_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-191-456171ba7ad5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Use torch.nn.Linear instead of weights and bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m##################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Mnist_Logistic_v2' object has no attribute 'lin'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are still able to use our same `fit` method as before."
      ],
      "metadata": {
        "id": "2ZGu1IYWACOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "NrHZx6WEAGYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9fbb3c-6ed1-4e1a-e6bf-e958b32ea7a6"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0594, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Using torch.optim"
      ],
      "metadata": {
        "id": "z9OmRODdDnc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch also has a package with various optimization algorithms, `torch.optim`.\n",
        "We can use the `step` method from our optimizer to take a forward step, instead\n",
        "of manually updating each parameter.\n",
        "\n",
        "This will let us replace our previous manually coded optimization step:\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "  for p in model.parameters(): p -= p.grad * lr\n",
        "  model.zero_grad()\n",
        "```\n",
        "\n",
        "and instead use just:\n",
        "\n",
        "```\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "```\n",
        "\n",
        "(`optim.zero_grad()` resets the gradient to 0 and we need to call it before\n",
        "computing the gradient for the next minibatch.)"
      ],
      "metadata": {
        "id": "-pLsb-RSAMZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "def get_model():\n",
        "  model = Mnist_Logistic()\n",
        "  return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    start_i = i * bs\n",
        "    end_i = start_i + bs\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "VQHSl8XuDw0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34ca77d-b3c6-4bf4-f759-6d00e8481e87"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2830, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0821, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Using Dataset and DataLoader"
      ],
      "metadata": {
        "id": "WTACVxjSDqme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch has an abstract Dataset class.  A Dataset can be anything that has\n",
        "a `__len__` function (called by Python's standard ``len`` function) and\n",
        "a `__getitem__` function as a way of indexing into it.\n",
        "`This tutorial <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_\n",
        "walks through a nice example of creating a custom `FacialLandmarkDataset` class\n",
        "as a subclass of `Dataset`.\n",
        "\n",
        "PyTorch's `TensorDataset <https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset>`_\n",
        "is a Dataset wrapping tensors. By defining a length and way of indexing,\n",
        "this also gives us a way to iterate, index, and slice along the first\n",
        "dimension of a tensor. This will make it easier to access both the\n",
        "independent and dependent variables in the same line as we train."
      ],
      "metadata": {
        "id": "CWH3JKaIAiCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "PTvV2jZ1Dxf-"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both `x_train` and `y_train` can be combined in a single `TensorDataset`,\n",
        "which will be easier to iterate over and slice."
      ],
      "metadata": {
        "id": "jNBHy4DIAqfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(x_train, y_train)"
      ],
      "metadata": {
        "id": "jq3gsJ7aAwoU"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we had to iterate through minibatches of x and y values separately:\n",
        "\n",
        "```\n",
        "xb = x_train[start_i:end_i]\n",
        "yb = y_train[start_i:end_i]\n",
        "```\n",
        "\n",
        "Now, we can do these two steps together:\n",
        "\n",
        "```\n",
        "xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "```"
      ],
      "metadata": {
        "id": "BayksUIIAxHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range((n - 1) // bs + 1):\n",
        "    xb, yb = train_ds[i * bs: i * bs + bs]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "KAdo81o-A5vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f4ddcc-bc95-4756-ec35-2e8b16f3e48d"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0813, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch's `DataLoader` is responsible for managing batches. You can\n",
        "create a `DataLoader` from any `Dataset`. `DataLoader` makes it easier\n",
        "to iterate over batches. Rather than having to use `train_ds[i*bs : i*bs+bs]`,\n",
        "the DataLoader gives us each minibatch automatically."
      ],
      "metadata": {
        "id": "koShG4fwBA35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)"
      ],
      "metadata": {
        "id": "lN-rwampBJI9"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for xb, yb in train_dl:\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "ni9Sp9HZBWEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651e7440-8547-4f28-bc29-1b0060a747bf"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0819, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Add Validation and create `fit()`, `get_data()`"
      ],
      "metadata": {
        "id": "Ch_Bk6VoB6g_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we were just trying to get a reasonable training loop set up for\n",
        "use on our training data.  In reality, you **always** should also have\n",
        "a `validation set <https://www.fast.ai/2017/11/13/validation-sets/>`_, in order\n",
        "to identify if you are overfitting.\n",
        "\n",
        "Shuffling the training data is\n",
        "`important <https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks>`_\n",
        "to prevent correlation between batches and overfitting. On the other hand, the\n",
        "validation loss will be identical whether we shuffle the validation set or not.\n",
        "Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
        "\n",
        "We'll use a batch size for the validation set that is twice as large as\n",
        "that for the training set. This is because the validation set does not\n",
        "need backpropagation and thus takes less memory (it doesn't need to\n",
        "store the gradients). We take advantage of this to use a larger batch\n",
        "size and compute the loss more quickly."
      ],
      "metadata": {
        "id": "nhrFPnU2CM3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
      ],
      "metadata": {
        "id": "hg1ySIMqCQUu"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call `model.train()` before training, and `model.eval()`\n",
        "before inference, because these are used by layers such as `nn.BatchNorm2d`\n",
        "and `nn.Dropout` to ensure appropriate behaviour for these different phases.)\n",
        "\n"
      ],
      "metadata": {
        "id": "oMUyNYJCCS6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  for xb, yb in train_dl:\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
        "\n",
        "  print(epoch, valid_loss / len(valid_dl))"
      ],
      "metadata": {
        "id": "rEvPUI5pCYKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2901ea9-d61e-41cd-903d-7809dba1873f"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.2940)\n",
            "1 tensor(0.3026)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now do a little refactoring of our own. Since we go through a similar\n",
        "process twice of calculating the loss for both the training set and the\n",
        "validation set, let's make that into its own function, ``loss_batch``, which\n",
        "computes the loss for one batch.\n",
        "\n",
        "We pass an optimizer in for the training set, and use it to perform\n",
        "backprop.  For the validation set, we don't pass an optimizer, so the\n",
        "method doesn't perform backprop."
      ],
      "metadata": {
        "id": "Oi4dZzBrCXfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "  loss = loss_func(model(xb), yb)\n",
        "\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  return loss.item(), len(xb)"
      ],
      "metadata": {
        "id": "RMacH7i-CdBh"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fit` runs the necessary operations to train our model and compute the\n",
        "training and validation losses for each epoch."
      ],
      "metadata": {
        "id": "uVIE3xxCCg6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "      loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      losses, nums = zip(\n",
        "        *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "      )\n",
        "    val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "    print(epoch, val_loss)"
      ],
      "metadata": {
        "id": "sNmoIHsyCjo6"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_data` returns dataloaders for the training and validation sets."
      ],
      "metadata": {
        "id": "6k-AR5slCplb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(train_ds, valid_ds, bs):\n",
        "  return (\n",
        "    DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "    DataLoader(valid_ds, batch_size=bs * 2),\n",
        "  )"
      ],
      "metadata": {
        "id": "SwmLaesrCujV"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, our whole process of obtaining the data loaders and fitting the\n",
        "model can be run in 3 lines of code:"
      ],
      "metadata": {
        "id": "1NAZWO-xCxHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "model, opt = get_model()\n",
        "epochs=200\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "metadata": {
        "id": "zYkYLvECC0jb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59201cc7-327d-4bc8-8b9c-f4273c52f16c"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.3228580145120621\n",
            "1 0.36888434767723083\n",
            "2 0.2852396015882492\n",
            "3 0.32079715926647184\n",
            "4 0.2759931210517883\n",
            "5 0.27836070783734324\n",
            "6 0.29517745119333266\n",
            "7 0.3094207323849201\n",
            "8 0.301758221578598\n",
            "9 0.27219660264253615\n",
            "10 0.2892585129082203\n",
            "11 0.27698689678907396\n",
            "12 0.3380477472126484\n",
            "13 0.30124379326105116\n",
            "14 0.32295044162273406\n",
            "15 0.3176425820827484\n",
            "16 0.26779060262441634\n",
            "17 0.2746709809064865\n",
            "18 0.27111537048220635\n",
            "19 0.2793803441166878\n",
            "20 0.28428614817857745\n",
            "21 0.3003645017147064\n",
            "22 0.28074973423480987\n",
            "23 0.2856437356233597\n",
            "24 0.28982231882810594\n",
            "25 0.2704669677615166\n",
            "26 0.27342950330376625\n",
            "27 0.2915352580308914\n",
            "28 0.2805269512474537\n",
            "29 0.2962188400506973\n",
            "30 0.297270152759552\n",
            "31 0.2866364710688591\n",
            "32 0.27200536640882494\n",
            "33 0.2938590272188187\n",
            "34 0.2842643410682678\n",
            "35 0.2764585209488869\n",
            "36 0.29790636237859724\n",
            "37 0.28327860484719275\n",
            "38 0.2721197485148907\n",
            "39 0.2736191179692745\n",
            "40 0.28204030156135557\n",
            "41 0.27565662543177605\n",
            "42 0.3001819051861763\n",
            "43 0.2832891879618168\n",
            "44 0.2791833504676819\n",
            "45 0.32398991522789\n",
            "46 0.3144569744467735\n",
            "47 0.27615292673110964\n",
            "48 0.29153869367837904\n",
            "49 0.294538964676857\n",
            "50 0.294326909327507\n",
            "51 0.2892041229248047\n",
            "52 0.2826776099562645\n",
            "53 0.28218752158880234\n",
            "54 0.28487028194069863\n",
            "55 0.27247667149305344\n",
            "56 0.2818229782223701\n",
            "57 0.29273436255455015\n",
            "58 0.28875885683298114\n",
            "59 0.2940144797921181\n",
            "60 0.2919941447734833\n",
            "61 0.2834047887802124\n",
            "62 0.31701101188063624\n",
            "63 0.3074871431350708\n",
            "64 0.2785333668112755\n",
            "65 0.29359869631528857\n",
            "66 0.29581910749673845\n",
            "67 0.3016272236824036\n",
            "68 0.2865792256653309\n",
            "69 0.3061052528679371\n",
            "70 0.2935168106198311\n",
            "71 0.3107599223613739\n",
            "72 0.3269100160956383\n",
            "73 0.29153925288915633\n",
            "74 0.2973782552242279\n",
            "75 0.30587955395579336\n",
            "76 0.3354111959338188\n",
            "77 0.307603815305233\n",
            "78 0.3001110486984253\n",
            "79 0.29257040045261384\n",
            "80 0.289994524371624\n",
            "81 0.3270343070268631\n",
            "82 0.2891423363029957\n",
            "83 0.2941117117404938\n",
            "84 0.2936189459085464\n",
            "85 0.2836709212183952\n",
            "86 0.28079453199505805\n",
            "87 0.28837758213281633\n",
            "88 0.31661450194120405\n",
            "89 0.30647812759876253\n",
            "90 0.3153620711445808\n",
            "91 0.28400612832307814\n",
            "92 0.3605794565439224\n",
            "93 0.30047086923122407\n",
            "94 0.27997780883908274\n",
            "95 0.2980311596751213\n",
            "96 0.28741774748563764\n",
            "97 0.33278644451498984\n",
            "98 0.2892628233611584\n",
            "99 0.29019874795675277\n",
            "100 0.2868818022429943\n",
            "101 0.2946178706407547\n",
            "102 0.2970051279783249\n",
            "103 0.317811881172657\n",
            "104 0.2872482021212578\n",
            "105 0.2889849538087845\n",
            "106 0.3535002787590027\n",
            "107 0.3042933634519577\n",
            "108 0.3174024395108223\n",
            "109 0.28212366843223574\n",
            "110 0.28997567937374114\n",
            "111 0.28537616470456123\n",
            "112 0.3434886844575405\n",
            "113 0.28826008275151255\n",
            "114 0.3575913644015789\n",
            "115 0.317408713054657\n",
            "116 0.29040371562242506\n",
            "117 0.2880147376716137\n",
            "118 0.2839758302092552\n",
            "119 0.28426145460605623\n",
            "120 0.28527212386131284\n",
            "121 0.28923950847387314\n",
            "122 0.29639989317655563\n",
            "123 0.31537705332636834\n",
            "124 0.2945394725084305\n",
            "125 0.294082594794035\n",
            "126 0.31278774618506433\n",
            "127 0.30213702037334444\n",
            "128 0.31406646149754525\n",
            "129 0.285690807902813\n",
            "130 0.28612825355529786\n",
            "131 0.2920976736545563\n",
            "132 0.29208589763641357\n",
            "133 0.28463674299120906\n",
            "134 0.32391308909654615\n",
            "135 0.2935713750720024\n",
            "136 0.2910332119643688\n",
            "137 0.37698779051303866\n",
            "138 0.2972323679327965\n",
            "139 0.3006424267351627\n",
            "140 0.30279466868638993\n",
            "141 0.2984298095703125\n",
            "142 0.28986789748668673\n",
            "143 0.3210065294504166\n",
            "144 0.31148135364055635\n",
            "145 0.3114183515250683\n",
            "146 0.3150019418001175\n",
            "147 0.3161204961001873\n",
            "148 0.33406476162672044\n",
            "149 0.3561509542584419\n",
            "150 0.35612659192085266\n",
            "151 0.2978169144272804\n",
            "152 0.3035924794971943\n",
            "153 0.28748550544977186\n",
            "154 0.3139162364065647\n",
            "155 0.31217138235569\n",
            "156 0.2907730112075806\n",
            "157 0.2924221861600876\n",
            "158 0.29289193471074104\n",
            "159 0.30318596708774564\n",
            "160 0.3002296290636063\n",
            "161 0.2913516988873482\n",
            "162 0.3340410184979439\n",
            "163 0.29032512432932855\n",
            "164 0.2890873477101326\n",
            "165 0.35612649183273315\n",
            "166 0.3328419510006905\n",
            "167 0.30592748461961744\n",
            "168 0.2990450968146324\n",
            "169 0.2928151299595833\n",
            "170 0.2958675647497177\n",
            "171 0.2915518993973732\n",
            "172 0.30094433069229126\n",
            "173 0.3005999030470848\n",
            "174 0.3042461634755135\n",
            "175 0.30178949105739594\n",
            "176 0.32457999984025954\n",
            "177 0.2994857750535011\n",
            "178 0.2937752175331116\n",
            "179 0.3188586188673973\n",
            "180 0.29265900982022286\n",
            "181 0.3077790665626526\n",
            "182 0.30300104451179505\n",
            "183 0.29980621931552887\n",
            "184 0.3290550636231899\n",
            "185 0.3173308723449707\n",
            "186 0.29821691695451735\n",
            "187 0.3005223601102829\n",
            "188 0.29429880878925324\n",
            "189 0.288459887778759\n",
            "190 0.32194144632816313\n",
            "191 0.3665773195266724\n",
            "192 0.3116550213843584\n",
            "193 0.2929554304122925\n",
            "194 0.3047728111624718\n",
            "195 0.3011463732123375\n",
            "196 0.30267915136814116\n",
            "197 0.29566810400485993\n",
            "198 0.30452367469072344\n",
            "199 0.29248425399661065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test"
      ],
      "metadata": {
        "id": "hA2v43rsGVSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create test dataloader,"
      ],
      "metadata": {
        "id": "KezydESjHzgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dl = DataLoader(valid_ds, batch_size=1)"
      ],
      "metadata": {
        "id": "Qd7RmASHHp73"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict test images, and visualize wrong samples."
      ],
      "metadata": {
        "id": "osaM6D0JIVR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for xb, yb in valid_dl:\n",
        "    ##################\n",
        "    # Get predict label\n",
        "    # CODE HERE\n",
        "    out = model(xb)\n",
        "    pred = torch.argmax(out, dim=1)\n",
        "    #\n",
        "    ##################\n",
        "\n",
        "    ##################\n",
        "    # Visualize sample if it is wrong\n",
        "    # CODE HERE\n",
        "    if pred != yb:\n",
        "        print('groundtruth: ', yb.numpy())\n",
        "        print(\"prediction: \", pred.numpy())\n",
        "        print(\"image:\")\n",
        "        pyplot.imshow(xb[0].numpy().reshape((28,28)), cmap = 'gray')\n",
        "        break\n",
        "    #\n",
        "    ##################"
      ],
      "metadata": {
        "id": "EJFjV3a7ITtT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "136feb0d-6060-4f5b-e27b-c61752655920"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groundtruth:  [5]\n",
            "prediction:  [8]\n",
            "image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}